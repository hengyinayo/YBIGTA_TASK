{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c1cdf5",
   "metadata": {
    "id": "c2c1cdf5"
   },
   "source": [
    "# 필요한 모듈 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "F65VNN-Ib0Zr",
   "metadata": {
    "id": "F65VNN-Ib0Zr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium==4.8.2 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (4.8.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from selenium==4.8.2) (2023.5.7)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from selenium==4.8.2) (1.26.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from selenium==4.8.2) (0.10.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from selenium==4.8.2) (0.22.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium==4.8.2) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium==4.8.2) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium==4.8.2) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium==4.8.2) (1.2.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium==4.8.2) (22.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium==4.8.2) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium==4.8.2) (1.1.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium==4.8.2) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium==4.8.2) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium==4.8.2) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium==4.8.2) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium==4.8.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2307d201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver_manager\n",
      "  Using cached webdriver_manager-3.8.6-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from webdriver_manager) (4.64.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from webdriver_manager) (22.0)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\wuhen\\anaconda3\\lib\\site-packages (from tqdm->webdriver_manager) (0.4.6)\n",
      "Installing collected packages: python-dotenv, webdriver_manager\n",
      "Successfully installed python-dotenv-1.0.0 webdriver_manager-3.8.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b95274c-2050-4f8e-a4ee-866655f39b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141.0\n",
      "3.8.6\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18f49a25",
   "metadata": {
    "id": "18f49a25",
    "outputId": "a41a7122-255b-48c0-f40b-4975739af304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141.0\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "print(selenium.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546c144",
   "metadata": {
    "id": "f546c144"
   },
   "source": [
    "# Selenium을 이용하여 브라우저 열기, 클릭, 페이지 이동, 닫기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4212ba4",
   "metadata": {
    "id": "a4212ba4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ChromeOptions 객체를 생성하고 이를 options에 저장합니다. ChromeOptions 객체는 웹 드라이버에 대한 다양한 설정을 관리하는데 사용됩니다. 이를 통해 예를 들어 헤드리스 모드로 실행하거나 특정 확장 프로그램을 로드하는 등의 설정을 할 수 있습니다.\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# # Selenium이 접근하기 어려운 웹페이지에 접근하게 해줌\n",
    "# options.add_argument('--no-sandbox')\n",
    "# # Shared memory를 사용하게 함\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# WebDriver는 웹 페이지를 자동화하기 위한 인터페이스를 제공하는 클래스입니다. webdriver.Chrome()는 Chrome 브라우저를 컨트롤하기 위한 WebDriver 객체를 생성합니다. 이때, 첫번째 인자로 ChromeDriver의 경로를 받습니다. 이 경로를 자동으로 관리해주는 것이 ChromeDriverManager().install()입니다. ChromeDriverManager는 ChromeDriver의 최신 버전을 자동으로 다운로드하고 설치해주며, 그 경로를 반환해줍니다. 또한, options=options로 설정한 ChromeOptions 객체를 전달하여 웹 드라이버의 설정을 변경합니다.\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options) # 코드 돌릴때마다 자동으로 설정해줘서 오류가 잘 안 뜸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf2718ab",
   "metadata": {
    "id": "bf2718ab"
   },
   "outputs": [],
   "source": [
    "# 네이버 웹툰 - 토요일\n",
    "url = 'https://comic.naver.com/webtoon?tab=sat' \n",
    "driver.get(url)\n",
    "driver.maximize_window() # 브라우저의 크기 전체화면으로 확대"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a486c4b",
   "metadata": {
    "id": "8a486c4b"
   },
   "outputs": [],
   "source": [
    "sunday_webtoon_button = driver.find_element(By.XPATH, '//*[@id=\"wrap\"]/header/div[3]/nav/ul/li[8]/a') # 일요일 웹툰 버튼 클릭\n",
    "sunday_webtoon_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71829c6b",
   "metadata": {
    "id": "71829c6b"
   },
   "outputs": [],
   "source": [
    "driver.back() # 앞의 페이지로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b8188b3",
   "metadata": {
    "id": "6b8188b3"
   },
   "outputs": [],
   "source": [
    "driver.forward() # 뒤의 페이지로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ea38726",
   "metadata": {
    "id": "6ea38726"
   },
   "outputs": [],
   "source": [
    "driver.close() # 브라우저 창 닫기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae1db2",
   "metadata": {
    "id": "63ae1db2"
   },
   "source": [
    "# 네이버 토, 일 웹툰 정보 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3527ca4",
   "metadata": {
    "id": "c3527ca4",
    "outputId": "c940262b-fef5-4475-f8b0-37f437cf16aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "2번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "3번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "4번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "5번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "6번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "7번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "8번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "9번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "10번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "11번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "12번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "13번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "14번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "15번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "16번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "17번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "18번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "19번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "20번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "21번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "22번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "23번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "24번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "25번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "26번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "27번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "28번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "29번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "30번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "31번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "32번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "33번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "34번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "35번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "36번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "37번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "38번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "39번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "40번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "41번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "42번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "43번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "44번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "45번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "46번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "47번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "48번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "49번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "50번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "51번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "52번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "53번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "54번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "55번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "56번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "57번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "58번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "59번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "60번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "61번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "62번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "63번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "64번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "65번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "66번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "67번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "68번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "69번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "70번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "71번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "72번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "73번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "74번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "75번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "76번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "77번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "78번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n",
      "79번째 웹툰정보 크롤링\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m sunday_webtoon_button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m    108\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m(values, columns \u001b[38;5;241m=\u001b[39m columns)\n\u001b[0;32m    111\u001b[0m webtoon_list\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'DataFrame'"
     ]
    }
   ],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "webtoon_list = [] # 토요일 일요일 데이터 프레임을 담을 리스트\n",
    "weekend_dic = {0:'토요일', 1:'일요일'}\n",
    "url = 'https://comic.naver.com/webtoon?tab=sat' # 우리가 갈 url\n",
    "driver.get(url) \n",
    "driver.implicitly_wait(2) # 로딩이 잘 될때까지 기다림\n",
    "\n",
    "########## 옵션임, window창을 띄워주는 역할\n",
    "try:\n",
    "    driver.maximize_window()\n",
    "except:\n",
    "    pass\n",
    "##########\n",
    "for weekend in range(2):\n",
    "    # 페이지를 가장 아래까지 스크롤하는 방법\n",
    "\n",
    "    interval = 1 # 1초에 한번씩 스크롤 내림\n",
    "\n",
    "############### 스크롤, 자동으로 페이지 맨 아래까지 스크롤\n",
    "    # 현재 문서 높이를 가져와서 저장 (현재 웹페이지의 전체 높이(스크롤 가능한 높이)를 픽셀 단위로 반환\n",
    "    # document.body.scrollHeight (자바스크립트 코드)는 문서의 전체 높이를 나타낸다. \n",
    "    # 스크롤이 가능한 경우, 이 값은 보이는 viewport의 높이보다 클 수 있다.\n",
    "    prev_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # 반복 수행\n",
    "    while True:\n",
    "        # 스크롤을 가장 아래로 내림\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        # time.sleep(interval)\n",
    "        # driver.execute_script(\"window.scrollTo(0, 0)\")\n",
    "        # break\n",
    "        \n",
    "        # 페이지 로딩 대기\n",
    "        time.sleep(interval)\n",
    "\n",
    "        # 현재 문서 높이를 가져와서 저장\n",
    "        curr_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if curr_height == prev_height:\n",
    "            break\n",
    "\n",
    "        prev_height = curr_height\n",
    "###############\n",
    "\n",
    "\n",
    "    # 해당 Xpath 정보 (개발자 창 열고, 마우스로 지정하고, 우클릭, copy)를 확인할때까지 5초간 기다림, 5초후에도 해당 Xpath를 찾지 못하면 에러 반환\n",
    "    # elem = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"content\"]/div[1]/ul/li[40]/a/div/img')))\n",
    "\n",
    "    columns = ['weekend', 'name', 'author', 'rate']         # 웹툰이름, 웹툰작가, 웹툰평점, 정보 크롤링\n",
    "    values = []                                             # 토요일 웹툰 리스트와 일요일 웹툰 리스트 정보를 담을 빈 리스트 생성\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')        # 현재 브라우저의 page의 html 정보를 lxml을 이용하여 가져오기 (\"driver.page_source\" 이 쪽에 페이지소스를 넣는거)\n",
    "    \n",
    "    data_rows = soup.find_all('li', attrs={'class':'item'}) # 해당 조건에 맞는 정보들을 list로 반환 (웹툰 사이트의 각 웹툰들 <div class=\"component_wrap\"> 부분\n",
    "\n",
    "    for i, row in enumerate(data_rows):                     # data_rows 리스트에 있는 값들과 인덱스를 차례대로 가져온다\n",
    "\n",
    "        print('{}번째 웹툰정보 크롤링'.format(i+1))\n",
    "        blank = []\n",
    "        blank.append(weekend_dic[weekend]) # 여기 날짜 부분은 웹툰 페이지에 명시 안되어있어서 따로 써준거\n",
    "\n",
    "        name = row.find('span', attrs={'class':'ContentTitle__title--e3qXt'})       # 웹툰이름 가져오기\n",
    "        if name:                                                                    # 해당 조건에 맞는 정보가 하나라도 있을 경우:\n",
    "            name = name.get_text().strip()                                          # html에서 텍스트만 가져온 뒤 공백 제거\n",
    "            blank.append(name)\n",
    "        else:                                                                       # 해당 조건에 맞는 정보가 하나도 없을 경우:\n",
    "            blank.append('Something is wrong')\n",
    "            print('{}번째 웹툰이름 가져올때 문제발생'.format(i+1))\n",
    "            continue\n",
    "\n",
    "\n",
    "        author = row.find('a', attrs={'class':'ContentAuthor__author--CTAAP'})      # 웹툰작가 가져오기\n",
    "        if author:\n",
    "            author = author.get_text().strip()\n",
    "            blank.append(author)\n",
    "        else:\n",
    "            blank.append('Something is wrong')\n",
    "            print('{}번째 웹툰작가 가져올때 문제발생'.format(i+1))\n",
    "            continue\n",
    "\n",
    "\n",
    "        rate = row.find('span', attrs={'class':'Rating__star_area--dFzsb'})        # 웹툰평점 가져오기\n",
    "        if rate:\n",
    "            rate = rate.find('span', attrs={'class':'text'}).get_text().strip()    # class가 Rating__star_area--dFzsb인 span 태그 안에 있는 span 태그에서 텍스트 가져오기\n",
    "            blank.append(rate)\n",
    "        else:\n",
    "            blank.append('Something is wrong')\n",
    "            print('{}번째 웹툰평점 가져올때 문제발생'.format(i+1))\n",
    "            continue\n",
    "\n",
    "\n",
    "#         image_url = row.find('img', attrs={'class':'Poster__image--d9XTI'})     # 웹툰이름 가져오기\n",
    "#         if image_url:                                                           # 해당 조건에 맞는 정보가 하나라도 있을 경우\n",
    "#             image_url = image_url['src']                                        # src attribute의 값을 가져온다\n",
    "#             if image_url.startswith('//'):                                      # image_url이 https: 로 시작하지 않는 경우\n",
    "#                 image_url = 'https:' + image_url\n",
    "#             blank.append(image_url)\n",
    "#         else:\n",
    "#             blank.append('Something is wrong')\n",
    "#             print('{}번째 웹툰이미지 가져올때 문제발생'.format(i+1))\n",
    "#             continue\n",
    "\n",
    "        values.append(blank)\n",
    "        print('---------------------------------------------------')\n",
    "\n",
    "    sunday_webtoon_button = driver.find_element(By.XPATH, '//*[@id=\"wrap\"]/header/div[3]/nav/ul/li[8]/a') # 일요일 웹툰 클릭 (XPATH가 변동이 적음, 그래서 XPATH로 가져옴)\n",
    "    \n",
    "    sunday_webtoon_button.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    df = pd.DataFrame(values, columns = columns)\n",
    "    webtoon_list.append(df) # 최종적으로 [[토요일 웹툰 정보 리스트들], [일요을 웹툰 정보 리스트들]] 형태로 반환됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a5f3756",
   "metadata": {
    "id": "5a5f3756",
    "outputId": "902694b8-86a6-4487-e6d5-71cb8bb6a0eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekend</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>토요일</td>\n",
       "      <td>호랑이형님</td>\n",
       "      <td>이상규</td>\n",
       "      <td>9.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>토요일</td>\n",
       "      <td>99강화나무몽둥이</td>\n",
       "      <td>홍실 / 지페리</td>\n",
       "      <td>9.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>토요일</td>\n",
       "      <td>취사병 전설이 되다</td>\n",
       "      <td>제이로빈 / 이진수</td>\n",
       "      <td>9.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>토요일</td>\n",
       "      <td>물위의 우리</td>\n",
       "      <td>뱁새 / 왈패</td>\n",
       "      <td>9.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>토요일</td>\n",
       "      <td>프리드로우</td>\n",
       "      <td>전선욱</td>\n",
       "      <td>9.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>토요일</td>\n",
       "      <td>망나니 소교주로 환생했다</td>\n",
       "      <td>재무 / 전마두 / 대은호</td>\n",
       "      <td>9.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>토요일</td>\n",
       "      <td>마도전생기</td>\n",
       "      <td>포스스튜디오 / codezero</td>\n",
       "      <td>9.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>토요일</td>\n",
       "      <td>은탄</td>\n",
       "      <td>김규삼</td>\n",
       "      <td>9.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>토요일</td>\n",
       "      <td>귀환했는데 입대 전날이다</td>\n",
       "      <td>해일 / 오코믹 / 황금비둘기</td>\n",
       "      <td>9.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>토요일</td>\n",
       "      <td>작전명 순정</td>\n",
       "      <td>꼬까리 / 들덤</td>\n",
       "      <td>9.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  weekend           name             author  rate\n",
       "0     토요일          호랑이형님                이상규  9.92\n",
       "1     토요일      99강화나무몽둥이           홍실 / 지페리  9.81\n",
       "2     토요일     취사병 전설이 되다         제이로빈 / 이진수  9.87\n",
       "3     토요일         물위의 우리            뱁새 / 왈패  9.98\n",
       "4     토요일          프리드로우                전선욱  9.79\n",
       "5     토요일  망나니 소교주로 환생했다     재무 / 전마두 / 대은호  9.85\n",
       "6     토요일          마도전생기  포스스튜디오 / codezero  9.84\n",
       "7     토요일             은탄                김규삼  9.96\n",
       "8     토요일  귀환했는데 입대 전날이다   해일 / 오코믹 / 황금비둘기  9.90\n",
       "9     토요일         작전명 순정           꼬까리 / 들덤  9.96"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.concat(webtoon_list).reset_index(drop=True) # index 초기화 해줘야함\n",
    "df_.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e84645f",
   "metadata": {
    "id": "7e84645f",
    "outputId": "76867c76-b234-4bd1-80be-e29220ccf8b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df527c9",
   "metadata": {
    "id": "1df527c9"
   },
   "source": [
    "# Dataframe을 csv 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e12f8507",
   "metadata": {
    "id": "e12f8507"
   },
   "outputs": [],
   "source": [
    "df_.to_csv('네이버웹툰_토일_크롤링.csv', encoding = 'utf-8-sig') # 한글이 들어간 정보는 글자가 깨질 수 있어 encoding을 다음과 같이 지정하여 저장 (한글은 utf-8로 안해주면 깨짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f25af-bd21-4670-8857-69d81067d6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
